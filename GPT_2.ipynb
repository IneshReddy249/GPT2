{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "@dataclass\n",
        "class GPT2Config:\n",
        "    vocab_size: int = 50257\n",
        "    n_positions: int = 1024\n",
        "    n_embd: int = 768\n",
        "    n_layer: int = 12\n",
        "    n_head: int = 12\n",
        "\n",
        "    @property\n",
        "    def head_dim(self) -> int:\n",
        "        return self.n_embd // self.n_head\n",
        "\n",
        "config = GPT2Config()\n",
        "print(f\"Config: {config.n_layer} layers, {config.n_head} heads, {config.n_embd} dim\")"
      ],
      "metadata": {
        "id": "7RT9plgkyI-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.head_dim = config.head_dim\n",
        "\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=True)\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=True)\n",
        "        self.scale = 1.0 / math.sqrt(self.head_dim)\n",
        "\n",
        "    def forward(self, x, kv_cache=None):\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        qkv = self.c_attn(x)\n",
        "        q, k, v = qkv.split(self.n_embd, dim=-1)\n",
        "\n",
        "        q = q.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
        "        k = k.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        if kv_cache is not None:\n",
        "            k_cache, v_cache = kv_cache\n",
        "            k = torch.cat([k_cache, k], dim=2)\n",
        "            v = torch.cat([v_cache, v], dim=2)\n",
        "\n",
        "        new_cache = (k, v)\n",
        "\n",
        "        seq_len = k.shape[2]\n",
        "        mask = torch.tril(torch.ones(T, seq_len, device=x.device))\n",
        "        attn = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.masked_fill(mask == 0, float('-inf'))\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "        out = torch.matmul(attn, v)\n",
        "\n",
        "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        out = self.c_proj(out)\n",
        "        return out, new_cache"
      ],
      "metadata": {
        "id": "2hw8vuEeyNR3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=True)\n",
        "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = F.gelu(x, approximate='tanh')\n",
        "        x = self.c_proj(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "xK2iof_syPi2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x, kv_cache=None):\n",
        "        attn_out, new_cache = self.attn(self.ln_1(x), kv_cache)\n",
        "        x = x + attn_out\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x, new_cache"
      ],
      "metadata": {
        "id": "2PgcQpHIyR0A"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.wte = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "        self.wpe = nn.Embedding(config.n_positions, config.n_embd)\n",
        "        self.h = nn.ModuleList([Block(config) for _ in range(config.n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        self.lm_head.weight = self.wte.weight\n",
        "\n",
        "    def forward(self, input_ids, kv_caches=None, start_pos=0):\n",
        "        B, T = input_ids.shape\n",
        "\n",
        "        pos = torch.arange(start_pos, start_pos + T, device=input_ids.device)\n",
        "        tok_emb = self.wte(input_ids)\n",
        "        pos_emb = self.wpe(pos)\n",
        "        x = tok_emb + pos_emb\n",
        "\n",
        "        if kv_caches is None:\n",
        "            kv_caches = [None] * self.config.n_layer\n",
        "\n",
        "        new_caches = []\n",
        "        for i, block in enumerate(self.h):\n",
        "            x, new_cache = block(x, kv_caches[i])\n",
        "            new_caches.append(new_cache)\n",
        "\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "        return logits, new_caches\n",
        "\n",
        "model = GPT2(config)\n",
        "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
      ],
      "metadata": {
        "id": "iYvxGH-RyUNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_hf_weights(model, model_name=\"gpt2\"):\n",
        "    from transformers import GPT2LMHeadModel\n",
        "\n",
        "    hf_model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "    hf_sd = hf_model.state_dict()\n",
        "\n",
        "    transpose_keys = [\n",
        "        'attn.c_attn.weight', 'attn.c_proj.weight',\n",
        "        'mlp.c_fc.weight', 'mlp.c_proj.weight'\n",
        "    ]\n",
        "\n",
        "    our_sd = model.state_dict()\n",
        "\n",
        "    for key in our_sd.keys():\n",
        "        hf_key = key\n",
        "        if key.startswith('h.'):\n",
        "            hf_key = 'transformer.' + key\n",
        "        elif key in ['wte.weight', 'wpe.weight', 'ln_f.weight', 'ln_f.bias']:\n",
        "            hf_key = 'transformer.' + key\n",
        "        elif key == 'lm_head.weight':\n",
        "            hf_key = 'lm_head.weight'\n",
        "\n",
        "        if hf_key not in hf_sd:\n",
        "            continue\n",
        "\n",
        "        hf_tensor = hf_sd[hf_key]\n",
        "\n",
        "        needs_transpose = any(t in key for t in transpose_keys)\n",
        "        if needs_transpose and len(hf_tensor.shape) == 2:\n",
        "            hf_tensor = hf_tensor.T\n",
        "\n",
        "        if our_sd[key].shape != hf_tensor.shape:\n",
        "            continue\n",
        "\n",
        "        our_sd[key] = hf_tensor\n",
        "\n",
        "    model.load_state_dict(our_sd)\n",
        "    print(f\"Loaded weights from {model_name}\")\n",
        "    return model\n",
        "\n",
        "model = GPT2(config)\n",
        "model = load_hf_weights(model, \"gpt2\")\n",
        "model.eval()\n",
        "print(\"Model ready!\")"
      ],
      "metadata": {
        "id": "TCdFvvooyYkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model, prompt, max_new_tokens=50, device='cpu'):\n",
        "    from transformers import GPT2Tokenizer\n",
        "\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
        "    T = input_ids.shape[1]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits, kv_caches = model(input_ids, kv_caches=None, start_pos=0)\n",
        "        next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
        "        input_ids = torch.cat([input_ids, next_token], dim=1)\n",
        "\n",
        "        for i in range(max_new_tokens - 1):\n",
        "            logits, kv_caches = model(next_token, kv_caches=kv_caches, start_pos=T + i)\n",
        "            next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
        "            input_ids = torch.cat([input_ids, next_token], dim=1)\n",
        "\n",
        "            if next_token.item() == tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "    return tokenizer.decode(input_ids[0])\n",
        "\n",
        "output = generate(model, \"The meaning of life is\", max_new_tokens=30)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "VArZI3brycB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test KV cache is working\n",
        "input_ids = torch.randint(0, config.vocab_size, (1, 5))\n",
        "logits, caches = model(input_ids)\n",
        "print(f\"First pass - Input: {input_ids.shape}\")\n",
        "print(f\"Cache K shape: {caches[0][0].shape}\")  # [1, 12, 5, 64]\n",
        "\n",
        "next_token = torch.randint(0, config.vocab_size, (1, 1))\n",
        "logits, caches = model(next_token, kv_caches=caches, start_pos=5)\n",
        "print(f\"Cached pass - Input: {next_token.shape}\")\n",
        "print(f\"Cache K shape: {caches[0][0].shape}\")  # [1, 12, 6, 64] - grew by 1!"
      ],
      "metadata": {
        "id": "CZwCiKMNyfQH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}